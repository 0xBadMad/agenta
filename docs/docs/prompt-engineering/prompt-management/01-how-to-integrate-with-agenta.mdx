---
title: "Integrating with agenta"
description: "Integrate applications and prompts created in Agenta into your projects."
---

import Image from "@theme/IdealImage";

Agenta easily integrates with your workflow, allowing you to use the latest version of the deployed prompt in your application. With Agenta, **you can update prompts directly from the web interface without modifying your code** each time.

Here are the two ways you can use the prompts from Agenta in your code:

### [1. As a prompt management system](/prompt-engineering/prompt-management/prompt-management-sdk):

In this approach, prompts are managed and stored in the Agenta backend. You use the Agenta SDK to fetch the latest deployed version of your prompt and use it in your application.

**Advantages**:

- Agenta operates outside your application's critical path.
- Allows you to fetch and cache the latest prompt version for zero latency usage.

**Considerations**:

- You need to set up the [integration with observability](/observability/quickstart) yourself if you want to trace your calls for debugging and cost tracking.

<Image
  class="bg-white"
  img={require("/images/prompt_management/as-a-prompt-management.png")}
  loading="lazy"
  alt="A sequence diagram showing how to integrate with Agenta as a prompt management system"
/>



### **[2. As a middleware / model proxy](/prompt-engineering/prompt-management/proxy-calls)**:

In this setup, Agenta provides you with an endpoint that forwards requests to the LLM on your behalf.

**Advantages**:

- Simplified deployment.
- Automatic tracing without any changes to your code.

**Considerations**:

- Adds a slight latency to the response (approximately 0.3 seconds).
- Currently, we don't support streaming for these endpoints.

Overall, this approach is best suited for applications where latency isn't critical.

<Image
  class="bg-white"
  img={require("/images/prompt_management/as-a-proxy.png")}
  loading="lazy"
  alt="A sequence diagram showing how to integrate with Agenta as    a proxy"
/>

